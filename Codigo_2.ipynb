{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0cb49b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (111805284.py, line 47)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor page in link\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#third party advice\n",
    "\n",
    "\n",
    "# 1. Obtener la URL\n",
    "url = \"https://books.toscrape.com/\"\n",
    "respuesta_obtenida = requests.get(url)\n",
    "html_obtenido = respuesta_obtenida.text  # HTML como string\n",
    "\n",
    "# 2. Parsear ese HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(html_obtenido, 'html.parser')  \n",
    "\n",
    "#categorias\n",
    "\n",
    "# # --- MÃ©todo 1: usando ul_menu y for ---\n",
    "# categorias = soup.find('ul', class_='nav nav-list')\n",
    "# items = categorias.find_all('li')\n",
    "\n",
    "# diccionario_links = {}\n",
    "\n",
    "# for li in items:\n",
    "#     enlace = li.find('a')\n",
    "#     texto = enlace.text.strip()\n",
    "#     link = enlace['href']\n",
    "#     diccionario_links[texto] = link\n",
    "    \n",
    "# print('MÃ©todo 1:', diccionario_links)\n",
    "\n",
    "# # --- MÃ©todo 2: usando comprensiÃ³n de diccionario y select ---\n",
    "# diccionario_links2 = {a.text.strip(): a['href'] for a in soup.select('ul.nav.nav-list li a')}\n",
    "# print('MÃ©todo 2:', diccionario_links2)\n",
    "\n",
    "# --- MÃ©todo 3: usando un bucle for y select, pero en menos lÃ­neas ---\n",
    "\n",
    "diccionario_links3 = {}\n",
    "for a in soup.select('ul.nav.nav-list li a'):\n",
    "    diccionario_links3[a.text.strip()] = url + a['href']\n",
    "print(diccionario_links3)\n",
    "\n",
    "\n",
    "for link in diccionario_links3.keys():\n",
    "    link = requests.get(url)\n",
    "    link_text = link.text\n",
    "    if link_text.find('ul', class_ = 'pager') == True:\n",
    "        for page in link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ PÃ¡gina 1 -> 20 libros encontrados\n",
      "ðŸ“š A Light in the Attic | ðŸ’µ Ã‚Â£51.77 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "ðŸ“š Tipping the Velvet | ðŸ’µ Ã‚Â£53.74 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
      "ðŸ“š Soumission | ðŸ’µ Ã‚Â£50.10 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/soumission_998/index.html\n",
      "ðŸ“š Sharp Objects | ðŸ’µ Ã‚Â£47.82 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
      "ðŸ“š Sapiens: A Brief History of Humankind | ðŸ’µ Ã‚Â£54.23 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n",
      "ðŸ“š The Requiem Red | ðŸ’µ Ã‚Â£22.65 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/the-requiem-red_995/index.html\n",
      "ðŸ“š The Dirty Little Secrets of Getting Your Dream Job | ðŸ’µ Ã‚Â£33.34 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\n",
      "ðŸ“š The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull | ðŸ’µ Ã‚Â£17.93 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\n",
      "ðŸ“š The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics | ðŸ’µ Ã‚Â£22.60 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\n",
      "ðŸ“š The Black Maria | ðŸ’µ Ã‚Â£52.15 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/the-black-maria_991/index.html\n",
      "ðŸ“š Starving Hearts (Triangular Trade Trilogy, #1) | ðŸ’µ Ã‚Â£13.99 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\n",
      "ðŸ“š Shakespeare's Sonnets | ðŸ’µ Ã‚Â£20.66 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html\n",
      "ðŸ“š Set Me Free | ðŸ’µ Ã‚Â£17.46 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/set-me-free_988/index.html\n",
      "ðŸ“š Scott Pilgrim's Precious Little Life (Scott Pilgrim #1) | ðŸ’µ Ã‚Â£52.29 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\n",
      "ðŸ“š Rip it Up and Start Again | ðŸ’µ Ã‚Â£35.02 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html\n",
      "ðŸ“š Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991 | ðŸ’µ Ã‚Â£57.25 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\n",
      "ðŸ“š Olio | ðŸ’µ Ã‚Â£23.88 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/olio_984/index.html\n",
      "ðŸ“š Mesaerion: The Best Science Fiction Stories 1800-1849 | ðŸ’µ Ã‚Â£37.59 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\n",
      "ðŸ“š Libertarianism for Beginners | ðŸ’µ Ã‚Â£51.33 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html\n",
      "ðŸ“š It's Only the Himalayas | ðŸ’µ Ã‚Â£45.17 | ðŸ“¦ In stock | ðŸ”— https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html\n",
      "ðŸ“š It's Only the Himalayas | ðŸ’µ Ã‚Â£45.17 | ðŸ“¦ In stock | ðŸ“‚ Travel\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "libros = None\n",
    "\n",
    "\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "\n",
    "\n",
    "#funcion para recorrer todas las paginas \n",
    "\n",
    "for page in range(1, 2):  # Hay 50 pÃ¡ginas en total\n",
    "    url = base_url.format(page)\n",
    "    respuesta = requests.get(url)\n",
    "    soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "    # AquÃ­ haces scraping de los libros de esa pÃ¡gina\n",
    "    libros = soup.find_all('article', class_='product_pod')\n",
    "    print(f\"ðŸ“„ PÃ¡gina {page} -> {len(libros)} libros encontrados\")\n",
    "\n",
    "# recorrer_paginas()\n",
    "\n",
    "\n",
    "#   \n",
    "\n",
    "for libro in libros:\n",
    "    titulo = libro.h3.a['title']\n",
    "    precio = libro.find('p', class_='price_color').text\n",
    "    disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "    # Enlace al detalle del libro (relativo -> convertir a absoluto)\n",
    "    enlace_relativo = libro.h3.a['href']\n",
    "    enlace_absoluto = \"https://books.toscrape.com/catalogue/\" + enlace_relativo\n",
    "\n",
    "    print(f\"ðŸ“š {titulo} | ðŸ’µ {precio} | ðŸ“¦ {disponibilidad} | ðŸ”— {enlace_absoluto}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "detalle = requests.get(enlace_absoluto)\n",
    "soup_detalle = BeautifulSoup(detalle.text, 'html.parser')\n",
    "\n",
    "# CategorÃ­a: estÃ¡ en un breadcrumb (<ul class=\"breadcrumb\">)\n",
    "categoria = soup_detalle.find('ul', class_='breadcrumb').find_all('a')[-1].text\n",
    "\n",
    "print(f\"ðŸ“š {titulo} | ðŸ’µ {precio} | ðŸ“¦ {disponibilidad} | ðŸ“‚ {categoria}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f384c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c55ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000017E5D194050>, 'Connection to books.toscrape.com timed out. (connect timeout=30)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [WinError 10060] Se produjo un error durante el intento de conexiÃ³n ya que la parte conectada no respondiÃ³ adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexiÃ³n establecida ya que el host conectado no ha podido responder",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connection.py:207\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[32m    208\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    209\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.host\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.timeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m: (<urllib3.connection.HTTPSConnection object at 0x0000017E5D194050>, 'Connection to books.toscrape.com timed out. (connect timeout=30)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000017E5D194050>, 'Connection to books.toscrape.com timed out. (connect timeout=30)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectTimeout\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://books.toscrape.com/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Hacemos una peticiÃ³n GET a la pÃ¡gina principal para obtener el HTML\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m respuesta = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m soup = BeautifulSoup(respuesta.text, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Obtenemos todas las categorÃ­as disponibles (excepto 'All books') y armamos un diccionario con el nombre y el link absoluto\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gatoh\\Documents\\CODE PRO\\The Huddle\\4-SQL-webscraping\\venv\\Lib\\site-packages\\requests\\adapters.py:688\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    686\u001b[39m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, NewConnectionError):\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request=request)\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ResponseError):\n\u001b[32m    691\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n",
      "\u001b[31mConnectTimeout\u001b[39m: HTTPSConnectionPool(host='books.toscrape.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000017E5D194050>, 'Connection to books.toscrape.com timed out. (connect timeout=30)'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "\n",
    "\n",
    "# Definimos la URL base del sitio web a scrapear\n",
    "url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Hacemos una peticiÃ³n GET a la pÃ¡gina principal para obtener el HTML\n",
    "respuesta = requests.get(url,headers=headers, timeout=30)\n",
    "soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "# Obtenemos todas las categorÃ­as disponibles (excepto 'All books') y armamos un diccionario con el nombre y el link absoluto\n",
    "categorias = {\n",
    "    a.text.strip(): url + a['href'] \n",
    "    for a in soup.select('ul.nav.nav-list li a') \n",
    "    if a['href'] != 'catalogue/category/books_1/index.html'\n",
    "}  # Excluye 'All books' de la lista de categorÃ­as\n",
    "\n",
    "print(f\"CategorÃ­as encontradas: {len(categorias)}\")\n",
    "print(categorias)\n",
    "\n",
    "# Creamos una lista vacÃ­a donde guardaremos la informaciÃ³n de todos los libros\n",
    "libros_data = []\n",
    "\n",
    "# Recorremos cada categorÃ­a encontrada\n",
    "for nombre_cat, url_cat in categorias.items():\n",
    "    page = 1  # Empezamos por la primera pÃ¡gina de la categorÃ­a\n",
    "    while True:\n",
    "        # Armamos la URL de la pÃ¡gina actual de la categorÃ­a\n",
    "        if page == 1:\n",
    "            url_pagina = url_cat\n",
    "        else:\n",
    "            url_pagina = url_cat.replace('index.html', f'page-{page}.html')\n",
    "\n",
    "        # Hacemos la peticiÃ³n GET a la pÃ¡gina de la categorÃ­a\n",
    "        resp_cat = requests.get(url_pagina, timeout = 30)\n",
    "        soup_cat = BeautifulSoup(resp_cat.text, 'html.parser')\n",
    "\n",
    "        # Buscamos todos los libros en la pÃ¡gina actual\n",
    "        libros = soup_cat.find_all('article', class_='product_pod')\n",
    "\n",
    "        # Si no hay libros, significa que no hay mÃ¡s pÃ¡ginas en esta categorÃ­a\n",
    "        if not libros:\n",
    "            break\n",
    "\n",
    "        # Recorremos cada libro encontrado en la pÃ¡gina\n",
    "        for libro in libros:\n",
    "            # Extraemos el tÃ­tulo del libro\n",
    "            titulo = libro.h3.a['title']\n",
    "            # Extraemos el precio\n",
    "            precio = libro.find('p', class_='price_color').text\n",
    "            # Extraemos la disponibilidad (stock)\n",
    "            disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
    "            # Extraemos el rating (estrellas) como texto (ej: 'Three')\n",
    "            rating = libro.p['class'][1] if len(libro.p['class']) > 1 else 'None'\n",
    "            # Obtenemos el enlace relativo al detalle del libro\n",
    "            enlace_rel = libro.h3.a['href']\n",
    "            # Convertimos el enlace relativo a absoluto\n",
    "            if not enlace_rel.startswith('http'):\n",
    "                if enlace_rel.startswith('../'):\n",
    "                    enlace_rel = enlace_rel.replace('../', '')\n",
    "                enlace_abs = url + 'catalogue/' + enlace_rel\n",
    "            else:\n",
    "                enlace_abs = enlace_rel\n",
    "\n",
    "            # Hacemos una peticiÃ³n GET al detalle del libro para obtener mÃ¡s informaciÃ³n\n",
    "            detalle = requests.get(enlace_abs, timeout =30)\n",
    "            soup_detalle = BeautifulSoup(detalle.text, 'html.parser')\n",
    "\n",
    "            # Intentamos extraer la categorÃ­a desde el breadcrumb del detalle (por si difiere del nombre_cat)\n",
    "            try:\n",
    "                categoria = soup_detalle.find('ul', class_='breadcrumb').find_all('a')[-1].text.strip()\n",
    "            except:\n",
    "                categoria = nombre_cat\n",
    "\n",
    "            # Intentamos extraer el stock desde el detalle (por si hay mÃ¡s info)\n",
    "            try:\n",
    "                stock = soup_detalle.find('p', class_='instock availability').text.strip()\n",
    "            except:\n",
    "                stock = ''\n",
    "\n",
    "            # Guardamos toda la informaciÃ³n del libro en un diccionario\n",
    "            libro_info = {\n",
    "                'TÃ­tulo': titulo,\n",
    "                'Precio': precio,\n",
    "                'Disponibilidad': disponibilidad,\n",
    "                'Rating': rating,\n",
    "                'CategorÃ­a': categoria,\n",
    "                'Stock': stock,\n",
    "                'URL': enlace_abs\n",
    "            }\n",
    "\n",
    "            # Agregamos el diccionario a la lista de libros\n",
    "            libros_data.append(libro_info)\n",
    "\n",
    "            # Imprimimos la informaciÃ³n del libro en la terminal\n",
    "            print(libro_info)\n",
    "\n",
    "            # Pausamos un poco para no saturar el servidor\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Pasamos a la siguiente pÃ¡gina de la categorÃ­a\n",
    "        page += 1\n",
    "\n",
    "# Guardamos todos los libros recolectados en un archivo CSV\n",
    "with open('libros_books_to_scrape.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=libros_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(libros_data)\n",
    "\n",
    "print(f\"Total de libros guardados: {len(libros_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
